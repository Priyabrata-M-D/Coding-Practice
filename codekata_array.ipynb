{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dealer1\n"
     ]
    }
   ],
   "source": [
    "x=int(input())\n",
    "prices=list(map(int, input().split(' ')))\n",
    "best_prices=min(prices)\n",
    "best_dealer=prices.index(best_prices)\n",
    "print('Dealer'+str(best_dealer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 3 2\n"
     ]
    }
   ],
   "source": [
    "n = int(input())  # number of elements\n",
    "arr = list(map(int, input().split()))  # list of elements\n",
    "\n",
    "freq_dict = {}  # dictionary to store frequency of each element\n",
    "\n",
    "# loop to count the frequency of each element\n",
    "for num in arr:\n",
    "    if num in freq_dict:\n",
    "        freq_dict[num] += 1\n",
    "    else:\n",
    "        freq_dict[num] = 1\n",
    "\n",
    "# sort the elements based on frequency and then value\n",
    "sorted_arr = sorted(arr, key=lambda x: (freq_dict[x], x))\n",
    "\n",
    "# remove duplicates\n",
    "unique_arr = list(dict.fromkeys(sorted_arr))\n",
    "\n",
    "# print the result\n",
    "print(*unique_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=int(input())\n",
    "arr=list(map(int,input().split()))\n",
    "window_size=int(input())\n",
    "res=[]\n",
    "sample=arr[:window_size]\n",
    "for i in sample:\n",
    "    if i<0:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(input())\n",
    "arr = list(map(int, input().split(' ')))\n",
    "k = int(input())\n",
    "\n",
    "for i in range(n-k+1):\n",
    "    found_negative = False\n",
    "    for j in range(i, i+k):\n",
    "        if arr[j] < 0:\n",
    "            print(arr[j], end=\" \")\n",
    "            found_negative = True\n",
    "            break\n",
    "    if not found_negative:\n",
    "        print(0, end=\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import streamlit as st\n",
    "# from mongoconfi import upload_to_mongodb\n",
    "from tscrape import scrape_twitter_data\n",
    "from pymongo import MongoClient\n",
    "\n",
    "\n",
    "# Define the Streamlit app\n",
    "def main():\n",
    "    # Page title\n",
    "    st.title('Twitter Scraper')\n",
    "\n",
    "    # Get user input\n",
    "    hashtag = st.text_input('Hashtag/Keyword')\n",
    "    start_date = st.date_input('Start Date')\n",
    "    end_date = st.date_input('End Date')\n",
    "    max_tweets = st.number_input('Max Tweets')\n",
    "\n",
    "    # Scrape Twitter data\n",
    "    if st.button('Scrape'):\n",
    "        tweets_df = scrape_twitter_data(hashtag, start_date, end_date, max_tweets)\n",
    "\n",
    "        # Display data in a table\n",
    "        st.write(tweets_df)\n",
    "\n",
    "        # Upload data to MongoDB\n",
    "        if st.button('Upload to MongoDB'):\n",
    "            # mongodb_conn_str = st.text_input('MongoDB Connection String')\n",
    "            # db_name = st.text_input('Database Name')\n",
    "            # collection_name = st.text_input('Collection Name')\n",
    "            client = MongoClient(\"mongodb://localhost:27017\")\n",
    "            db = client[\"twitter_scrapping\"]\n",
    "            collection = db[\"scrapped\"]\n",
    "            # Convert data to a list of dictionaries\n",
    "            data_dict = tweets_df.to_dict('records')\n",
    "            # Insert data into MongoDB\n",
    "            result = collection.insert_many(data_dict)\n",
    "            # result = upload_to_mongodb(tweets_df, mongodb_conn_str, db_name, collection_name)\n",
    "\n",
    "            # Display result\n",
    "            st.write(f'{len(result.inserted_ids)} documents uploaded to MongoDB')\n",
    "\n",
    "        # Download data in CSV format\n",
    "        csv = tweets_df.to_csv(index=False)\n",
    "        b64 = base64.b64encode(csv.encode()).decode()\n",
    "        st.markdown('### Download CSV File')\n",
    "        href = f'<a href=\"data:file/csv;base64,{b64}\" download=\"twitter_data.csv\">Download CSV</a>'\n",
    "        st.markdown(href, unsafe_allow_html=True)\n",
    "\n",
    "        # Download data in JSON format\n",
    "        json = tweets_df.to_json(orient='records')\n",
    "        b64 = base64.b64encode(json.encode()).decode()\n",
    "        st.markdown('### Download JSON File')\n",
    "        href = f'<a href=\"data:file/json;base64,{b64}\" download=\"twitter_data.json\">Download JSON</a>'\n",
    "        st.markdown(href, unsafe_allow_html=True)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ebad67f52a29b0206a12581b47023d96d9a8ba93c13e5c33ae278f26166e0551"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
